{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is Andrew's text, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"andrew's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing using string commands\n",
    "# needs text preprocessing before\n",
    "def text_token(text,sep=\" \"):\n",
    "    text = text.lower().split()\n",
    "    return text\n",
    "text_token(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing using sklearn built-in functions\n",
    "#needs text preprocessing before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 1 : ['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']\n",
      "Tokenizer 2 : ['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']\n",
      "Tokenizer 3 : ['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing using NLTK built-in functions\n",
    "from nltk.tokenize import WordPunctTokenizer , SpaceTokenizer , TreebankWordTokenizer \n",
    "\n",
    "Tokenizer_1 = WordPunctTokenizer()\n",
    "Tokenizer_2 = SpaceTokenizer()\n",
    "Tokenizer_3 = TreebankWordTokenizer()\n",
    "\n",
    "print(\"Tokenizer 1 :\",Tokenizer_1.tokenize(text)) \n",
    "print(\"Tokenizer 2 :\",Tokenizer_2.tokenize(text)) \n",
    "print(\"Tokenizer 3 :\",Tokenizer_3.tokenize(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wa walk with my friend , did n't i ?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "text = \"I was walking with my friends , didn't i ?\"\n",
    "Tokens = Tokenizer_3.tokenize(text)\n",
    "\n",
    "Stemmer = PorterStemmer()\n",
    "\n",
    "\" \".join(Stemmer.stem(token) for token in Tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I wa walking with my friend , did n't i ?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "Tokens = Tokenizer_3.tokenize(text)\n",
    "\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\" \".join(Lemmatizer.lemmatize(token) for token in Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split , KFold , RandomizedSearchCV \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle sentiment analysis dataset\n",
    "df = pd.read_csv(\"data/sen/train.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156040</th>\n",
       "      <td>156041</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156041</th>\n",
       "      <td>156042</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156042</th>\n",
       "      <td>156043</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156043</th>\n",
       "      <td>156044</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156044</th>\n",
       "      <td>156045</td>\n",
       "      <td>8544</td>\n",
       "      <td>with Herrmann quietly suggesting the sadness a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156045</th>\n",
       "      <td>156046</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann quietly suggesting the sadness and ob...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156046</th>\n",
       "      <td>156047</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156047</th>\n",
       "      <td>156048</td>\n",
       "      <td>8544</td>\n",
       "      <td>quietly suggesting the sadness and obsession b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156048</th>\n",
       "      <td>156049</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession beneath H...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156049</th>\n",
       "      <td>156050</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156050</th>\n",
       "      <td>156051</td>\n",
       "      <td>8544</td>\n",
       "      <td>the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156051</th>\n",
       "      <td>156052</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and obsession</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156052</th>\n",
       "      <td>156053</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156053</th>\n",
       "      <td>156054</td>\n",
       "      <td>8544</td>\n",
       "      <td>beneath Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156054</th>\n",
       "      <td>156055</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "156040    156041        8544   \n",
       "156041    156042        8544   \n",
       "156042    156043        8544   \n",
       "156043    156044        8544   \n",
       "156044    156045        8544   \n",
       "156045    156046        8544   \n",
       "156046    156047        8544   \n",
       "156047    156048        8544   \n",
       "156048    156049        8544   \n",
       "156049    156050        8544   \n",
       "156050    156051        8544   \n",
       "156051    156052        8544   \n",
       "156052    156053        8544   \n",
       "156053    156054        8544   \n",
       "156054    156055        8544   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "156040  is darkly atmospheric , with Herrmann quietly ...          2  \n",
       "156041  is darkly atmospheric , with Herrmann quietly ...          2  \n",
       "156042                            is darkly atmospheric ,          2  \n",
       "156043                              is darkly atmospheric          3  \n",
       "156044  with Herrmann quietly suggesting the sadness a...          2  \n",
       "156045  Herrmann quietly suggesting the sadness and ob...          2  \n",
       "156046                                           Herrmann          2  \n",
       "156047  quietly suggesting the sadness and obsession b...          1  \n",
       "156048  suggesting the sadness and obsession beneath H...          2  \n",
       "156049               suggesting the sadness and obsession          2  \n",
       "156050                          the sadness and obsession          2  \n",
       "156051                              sadness and obsession          1  \n",
       "156052                                        sadness and          1  \n",
       "156053        beneath Hearst 's forced avuncular chortles          2  \n",
       "156054                Hearst 's forced avuncular chortles          2  \n",
       "156055                                          Hearst 's          2  \n",
       "156056                          forced avuncular chortles          1  \n",
       "156057                                 avuncular chortles          3  \n",
       "156058                                          avuncular          2  \n",
       "156059                                           chortles          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(\"SentenceId\").mean()[\"Sentiment\"]\n",
    "df[\"new\"] = df[\"SentenceId\"].apply(lambda x : data[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>, I suspect ,</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>I suspect ,</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>I suspect</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>suspect</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0          1           1  A series of escapades demonstrating the adage ...   \n",
       "1          2           1  A series of escapades demonstrating the adage ...   \n",
       "2          3           1                                           A series   \n",
       "3          4           1                                                  A   \n",
       "4          5           1                                             series   \n",
       "..       ...         ...                                                ...   \n",
       "95        96           3                                      , I suspect ,   \n",
       "96        97           3                                        I suspect ,   \n",
       "97        98           3                                          I suspect   \n",
       "98        99           3                                                  I   \n",
       "99       100           3                                            suspect   \n",
       "\n",
       "    Sentiment       new  \n",
       "0           1  1.984127  \n",
       "1           2  1.984127  \n",
       "2           2  1.984127  \n",
       "3           2  1.984127  \n",
       "4           2  1.984127  \n",
       "..        ...       ...  \n",
       "95          2  1.714286  \n",
       "96          2  1.714286  \n",
       "97          2  1.714286  \n",
       "98          2  1.714286  \n",
       "99          2  1.714286  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.509945\n",
       "3    0.210989\n",
       "1    0.174760\n",
       "4    0.058990\n",
       "0    0.045316\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Phrase\"]=df[\"Phrase\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>a series</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  a series of escapades demonstrating the adage ...   \n",
       "1         2           1  a series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           a series   \n",
       "3         4           1                                                  a   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment       new  \n",
       "0          1  1.984127  \n",
       "1          2  1.984127  \n",
       "2          2  1.984127  \n",
       "3          2  1.984127  \n",
       "4          2  1.984127  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning Pipe line \n",
    "import re\n",
    "import string\n",
    "\n",
    "def pipeline(text):\n",
    "    \n",
    "    tokens = Tokenizer_3.tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = \" \".join(Lemmatizer.lemmatize(t) for t in stripped)\n",
    "    text = text.strip()\n",
    "    text = text.translate(remove_digits)\n",
    "    \n",
    "    return text\n",
    "df[\"Phrase\"]=df[\"Phrase\"].apply(lambda x : pipeline(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"Phrase\"].values\n",
    "y_train = df[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(stop_words=\"english\",ngram_range=(1,5))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_val , y_train , y_val = train_test_split(X,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier , RandomForestClassifier , GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression , RidgeClassifier \n",
    "from sklearn.metrics import accuracy_score , confusion_matrix \n",
    "from sklearn.naive_bayes import BernoulliNB , ClassifierMixin , GaussianNB , ComplementNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_1 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf):\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    preds_train = clf.predict(X_train)\n",
    "    print(\"accuracy :\",accuracy_score(y_val,preds))\n",
    "    print(\"accuracy_train :\",accuracy_score(y_train,preds_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.626393694732795\n",
      "accuracy_train : 0.707884787902089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/anaconda3/envs/deep_learning/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "evaluate(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
